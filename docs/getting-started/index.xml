<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>DELL Technologies â€“ Installation</title>
    <link>https://sakshimakkar.github.io/hugo1/docs/getting-started/</link>
    <description>Recent content in Installation on DELL Technologies</description>
    <generator>Hugo -- gohugo.io</generator>
    
	  <atom:link href="https://sakshimakkar.github.io/hugo1/docs/getting-started/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>Docs: Installation using Common helm Installer</title>
      <link>https://sakshimakkar.github.io/hugo1/docs/getting-started/helm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://sakshimakkar.github.io/hugo1/docs/getting-started/helm/</guid>
      <description>
        
        
        &lt;h2 id=&#34;installation-scripts-and-readmemd-files&#34;&gt;Installation scripts and README.md files&lt;/h2&gt;
&lt;p&gt;The scripts and documentation are located within the &lt;a href=&#34;installer&#34;&gt;installer&lt;/a&gt; directory. These scripts are designed
to be placed within the CSI Driver repos in a directory called &lt;code&gt;/dell-csi-helm-installer&lt;/code&gt; and require the driver helm chart
to be located within the &lt;code&gt;/helm&lt;/code&gt; directory of the same repo.&lt;/p&gt;
&lt;h2 id=&#34;copying-the-scripts-to-target-driver-repos&#34;&gt;Copying the scripts to target driver repos&lt;/h2&gt;
&lt;p&gt;A helper script is available that will copy the common installer scripts and docuemtnation to each driver. This script can be found at &lt;code&gt;util/update-driver.sh&lt;/code&gt;
and will perform the following steps for each driver:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Clone the driver repo&lt;/li&gt;
&lt;li&gt;Either checkout or create a branch for the changes to be applied to. If the user supplied branch name exists, it will be used. If it does not exist,
one will be created.&lt;/li&gt;
&lt;li&gt;Copy the common installer scripts to the target branch&lt;/li&gt;
&lt;li&gt;Copy the documentation to the target branch. While copying, it will substitute:&lt;/li&gt;
&lt;li&gt;The driver arrays formal name (PowerMax, PowerFlex, PowerStore, PowerScale, Unity) for any text matching __PROPERPRODUCTNAME__&lt;/li&gt;
&lt;li&gt;The driver arrays informal name, always in lower case (powermax, vxflexos, powerstore, isilon, unity) for text matching __LOWERPRODUCTNAME__&lt;/li&gt;
&lt;li&gt;After each file is copied, a &lt;code&gt;git diff&lt;/code&gt; will be performed so the user can inspect the changes and validate them&lt;/li&gt;
&lt;li&gt;Once all files are copied, the files will be added to a &lt;code&gt;git commit&lt;/code&gt; with a commit message that has been supplied by the user.&lt;/li&gt;
&lt;li&gt;At this stage, if there are any changes, the user will be prompted if they would like to &lt;code&gt;git push&lt;/code&gt; the changes to the git remote&lt;/li&gt;
&lt;li&gt;If the &lt;code&gt;git push&lt;/code&gt; is successful, the script can automatically create a PR in the target repository, if one does not exist. If the user elects to not create a PR, one can be created manually.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A video demonstrating the use of the &lt;code&gt;util/update-driver.sh&lt;/code&gt; script has been made, It can be accessed (Passcode: ftju8Np*) at &lt;a href=&#34;https://Dell.zoom.us/rec/share/qAOiNw3PSy1TtHBuzI9R6hN3W1PfZr_w8rKVk5X8Zg1axQqjYNYKG89mabbIkeef.9pLpkZfdJ3kEGKq4&#34;&gt;https://Dell.zoom.us/rec/share/qAOiNw3PSy1TtHBuzI9R6hN3W1PfZr_w8rKVk5X8Zg1axQqjYNYKG89mabbIkeef.9pLpkZfdJ3kEGKq4&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This script will create a temporary working directory that must be manually cleaned up. The location of this directory will be displayed at the end of the script.&lt;/p&gt;
&lt;p&gt;Help is available for this script&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[dell-csi-helm-installer]# update-driver.sh  -h
Help for update-driver.sh
This script will copy the Common Helm Installer scripts to a branch within each driver and push them to the remote

Usage: update-driver.sh options...
  --directory[=]&amp;lt;directory&amp;gt;            Directory to copy files from.
                                       Default is &amp;quot;/dell/git/dell-csi-helm-installer/installer&amp;quot;
  --branch[=]&amp;lt;branch&amp;gt;                  Branch to copy to. This branch will be created if it does not exist at the remote.
                                       Default is &amp;quot;feature/update-common-installer-scripts&amp;quot;
  --commit[=]&amp;lt;commit message&amp;gt;          Git commit message.
                                       Default is &amp;quot;Addressing installation review comments across all drivers&amp;quot;
  --assume-yes                         Assume &#39;yes&#39; for any questions. Default is not to assume this.
  -h                                   Help
&lt;/code&gt;&lt;/pre&gt;
      </description>
    </item>
    
    <item>
      <title>Docs: Installation using Operator</title>
      <link>https://sakshimakkar.github.io/hugo1/docs/getting-started/operator/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://sakshimakkar.github.io/hugo1/docs/getting-started/operator/</guid>
      <description>
        
        
        &lt;h2 id=&#34;description&#34;&gt;Description&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;dell-csi-operator&lt;/code&gt; is a Kubernetes operator which can be used to install and manage various CSI Drivers provided by Dell EMC for different storage arrays.&lt;/p&gt;
&lt;p&gt;It is built, deployed and tested using the toolset provided by Operator &lt;a href=&#34;https://github.com/operator-framework&#34;&gt;framework&lt;/a&gt; which include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/operator-framework/operator-sdk&#34;&gt;operator-sdk&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/operator-framework/operator-lifecycle-manager&#34;&gt;operator-lifecycle-manager&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/operator-framework/operator-registry&#34;&gt;operator-registry&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note - &lt;strong&gt;operator-sdk-vv1.18.2-x86_64-linux-gnu&lt;/strong&gt; has been used to build the &lt;code&gt;dell-csi-operator&lt;/code&gt; and is available at the root of the repository&lt;/p&gt;
&lt;h2 id=&#34;custom-resource-definitions&#34;&gt;Custom Resource Definitions&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;dell-csi-operator&lt;/code&gt; manages a set of &lt;a href=&#34;https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/&#34;&gt;Custom Resource Definitions&lt;/a&gt; (CRDs)&lt;br&gt;
These CRDs represent a specific CSI Driver installation and are part of the API group &lt;code&gt;storage.dell.com&lt;/code&gt;&lt;br&gt;
The current set of CRDs managed by the &lt;code&gt;dell-csi-operator&lt;/code&gt; are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CSIUnity&lt;/li&gt;
&lt;li&gt;CSIIsilon&lt;/li&gt;
&lt;li&gt;CSIVXFlexOS&lt;/li&gt;
&lt;li&gt;CSIPowerStore&lt;/li&gt;
&lt;li&gt;CSIPowerMax&lt;/li&gt;
&lt;li&gt;CSIPowerMaxRevProxy&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;controllers&#34;&gt;Controllers&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;dell-csi-operator&lt;/code&gt; utilizes Kubernetes &lt;a href=&#34;https://github.com/kubernetes-sigs/controller-runtime&#34;&gt;controller runtime&lt;/a&gt; libraries for building controllers which
run as part of the Operator deployment.&lt;br&gt;
These controllers watch for any requests to create/modify/delete instances of the Custom Resource Definitions (CRDs) and handle the &lt;a href=&#34;https://godoc.org/sigs.k8s.io/controller-runtime/pkg/reconcile&#34;&gt;reconciliation&lt;/a&gt;
of these requests.&lt;/p&gt;
&lt;p&gt;Each instance of a CRD is called a Custom Resource (CR) and can be managed by a client like &lt;code&gt;kubectl&lt;/code&gt; in the same way a native
Kubernetes resource is managed.&lt;br&gt;
When you create a Custom Resource, then the corresponding Controller will create the Kubernetes objects required for the driver installation.&lt;/p&gt;
&lt;p&gt;This includes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Service Accounts and RBAC configuration&lt;/li&gt;
&lt;li&gt;StatefulSet&lt;/li&gt;
&lt;li&gt;DaemonSet&lt;/li&gt;
&lt;li&gt;Deployment and Service (only for CSIPowerMaxRevProxy)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note - There is one controller per Custom Resource type and each controller runs a single worker&lt;/p&gt;
&lt;h2 id=&#34;build-and-deploy&#34;&gt;Build and Deploy&lt;/h2&gt;
&lt;h3 id=&#34;pre-requisites&#34;&gt;Pre-requisites&lt;/h3&gt;
&lt;p&gt;Make sure you are running the build on a machine with go &lt;strong&gt;1.13.x&lt;/strong&gt; installed&lt;/p&gt;
&lt;p&gt;There are multiple Makefile targets available for building the Operator&lt;/p&gt;
&lt;h3 id=&#34;building-operator-binary&#34;&gt;Building Operator binary&lt;/h3&gt;
&lt;p&gt;If you wish to build the Operator binary, run the command - &lt;code&gt;make go-build&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;build-operator-image&#34;&gt;Build Operator image&lt;/h3&gt;
&lt;p&gt;There are a few available Makefile targets which let you build a docker image for the Operator.
The docker image is built using the &lt;code&gt;operator-sdk&lt;/code&gt; binary (which is available in the repository).
The base image used to build the docker image is UBI (Universal Base Image) provided by Red Hat.&lt;/p&gt;
&lt;p&gt;Run the command &lt;code&gt;make docker-local&lt;/code&gt; to make a docker image for the operator which will be tagged
with the current logged in user and a timestamp. This option is useful for multiple developers using the same testing environment&lt;/p&gt;
&lt;p&gt;Run the command &lt;code&gt;make docker&lt;/code&gt; to build a docker image for the Operator which will be tagged with git semantic versioning&lt;br&gt;
The official builds of Operator which are hosted on artifactory are built using the &lt;code&gt;make docker&lt;/code&gt; command&lt;/p&gt;
&lt;p&gt;By default, this target will tag the newly built images with the artifactory repo&lt;br&gt;
Run the command &lt;code&gt;REGISTRY=my-image-repo/dell-csi-operator make docker&lt;/code&gt; to tag the docker image with your own repository&lt;/p&gt;
&lt;h3 id=&#34;push-docker-image-to-private-repository&#34;&gt;Push docker image to private repository&lt;/h3&gt;
&lt;p&gt;Run the command &lt;code&gt;REGISTRY=my-image-repo/dell-csi-operator make docker-push&lt;/code&gt; to build and push the docker image to &lt;code&gt;my-image-repo&lt;/code&gt; private repository&lt;/p&gt;
&lt;h3 id=&#34;push-docker-image-to-artifactory&#34;&gt;Push docker image to artifactory&lt;/h3&gt;
&lt;p&gt;Run  the command &lt;code&gt;make docker-push&lt;/code&gt; to build and push the docker image to the artifactory repository for Operator&lt;/p&gt;
&lt;p&gt;Note - Don&amp;rsquo;t use this option unless you really wish to push the image to artifactory as it may end up overwriting official builds&lt;/p&gt;
&lt;h3 id=&#34;run-the-operator-locally-without-deploying-any-image&#34;&gt;Run the Operator locally without deploying any image&lt;/h3&gt;
&lt;h4 id=&#34;pre-requisites-1&#34;&gt;Pre-requisites&lt;/h4&gt;
&lt;p&gt;Make sure that a &lt;a href=&#34;https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/&#34;&gt;&lt;strong&gt;KubeConfig&lt;/strong&gt;&lt;/a&gt; file pointing to your Kubernetes/OpenShift cluster is present in the default location&lt;/p&gt;
&lt;p&gt;Run either of the following commands to run the Operator in your cluster without creating a deployment&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;make go-run&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;./operator-sdk run local&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note - Make sure to install the CRDs managed by the Operator before running it locally&lt;/p&gt;
&lt;p&gt;The above command will run the Operator binary which will be equivalent to installing the operator in the default namespace&lt;/p&gt;
&lt;h4 id=&#34;deploy-the-operator&#34;&gt;Deploy the operator&lt;/h4&gt;
&lt;p&gt;There are primarily two ways of deploying the Operator -&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Use Operator manifests and installation scripts&lt;/li&gt;
&lt;li&gt;Use &lt;a href=&#34;https://github.com/operator-framework/operator-lifecycle-manager&#34;&gt;OLM&lt;/a&gt; to install the Operator&lt;br&gt;
During development -&lt;br&gt;
a) Use internal Operator Catalog hosted on artifactory&lt;br&gt;
b) Use OperatorHub to deploy publicly available Operator&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;deploy-operator-without-olm&#34;&gt;Deploy Operator without OLM&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Update the docker image in the file &lt;code&gt;deploy/operator.yaml&lt;/code&gt; to the one you wish to install&lt;br&gt;
This could be an image from artifactory, an image which you built or any public image on dockerhub&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Run the command &lt;code&gt;bash scripts/install.sh&lt;/code&gt; to deploy the Operator in the default namespace&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The installation script does the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create a Service Account and setup the RBAC using ClusterRole and ClusterRoleBindings&lt;/li&gt;
&lt;li&gt;Create a ConfigMap which is used by the Operator&lt;/li&gt;
&lt;li&gt;Install the various CRDs managed by the Operator&lt;/li&gt;
&lt;li&gt;Create the Operator deployment (single replica)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;deploy-operator-using-olm&#34;&gt;Deploy Operator using OLM&lt;/h3&gt;
&lt;h4 id=&#34;pre-requisites-2&#34;&gt;Pre-requisites&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/operator-framework/operator-lifecycle-manager&#34;&gt;OLM&lt;/a&gt; is not available on upstream Kubernetes cluster by default and has to be installed before installing the Operator.&lt;/p&gt;
&lt;p&gt;Run the  command &lt;code&gt;./operator-sdk olm install&lt;/code&gt; to install &lt;code&gt;OLM&lt;/code&gt; in your cluster&lt;/p&gt;
&lt;p&gt;Note - OLM is available as a default component in OpenShift clusters and you don&amp;rsquo;t need to install it separately&lt;/p&gt;
&lt;p&gt;Note - Please refer Operator SDK &lt;a href=&#34;https://sdk.operatorframework.io/docs/&#34;&gt;documentation&lt;/a&gt; for more help on using operator-sdk&lt;/p&gt;
&lt;h4 id=&#34;deploy-operator-using-internal-catalog&#34;&gt;Deploy Operator using internal Catalog&lt;/h4&gt;
&lt;p&gt;Run the command &lt;code&gt;bash scripts/install_olm.sh&lt;/code&gt; to install the Operator using OLM in an upstream Kubernetes cluster
Run the command &lt;code&gt;bash scripts/install_olm.sh --openshift&lt;/code&gt; to install the Operator using OLM in an OpenShift cluster&lt;/p&gt;
&lt;p&gt;The above scripts will create instances of the following CRDs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CatalogSource&lt;/li&gt;
&lt;li&gt;OperatorGroup&lt;/li&gt;
&lt;li&gt;Subscription&lt;/li&gt;
&lt;li&gt;InstallPlan&lt;/li&gt;
&lt;li&gt;ClusterServiceVersion&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can query the status of these objects in the &lt;code&gt;test-olm&lt;/code&gt; namespace to get more information or troubleshoot any issues found during installation.&lt;/p&gt;
&lt;h3 id=&#34;verify-installation&#34;&gt;Verify Installation&lt;/h3&gt;
&lt;p&gt;Post a successful installation of the Operator, there should be an Operator deployment created in the cluster.&lt;br&gt;
Depending on the method of installation, this would be created in the namespace:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Installation without OLM - &lt;code&gt;default&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Installation with OLM - &lt;code&gt;test-olm&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;query-for-crds&#34;&gt;Query for CRDs&lt;/h4&gt;
&lt;p&gt;You can also query for the CRDs installed in the cluster by running the command
&lt;code&gt;kubectl get crd&lt;/code&gt;. You should see the following CRDs in the list of CRDs installed in the cluster:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;csiisilons.storage.dell.com&lt;/li&gt;
&lt;li&gt;csipowermaxes.storage.dell.com&lt;/li&gt;
&lt;li&gt;csipowermaxrevproxies.storage.dell.com&lt;/li&gt;
&lt;li&gt;csipowerstores.storage.dell.com&lt;/li&gt;
&lt;li&gt;csiunities.storage.dell.com&lt;/li&gt;
&lt;li&gt;csivxflexoses.storage.dell.com&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;install-a-csi-driver&#34;&gt;Install a CSI driver&lt;/h2&gt;
&lt;p&gt;Once the CRDs and Operator has been installed in the cluster, you can install any CSI driver by creating a Custom Resource (CR)&lt;br&gt;
For e.g. - If you want to install the CSI PowerMax driver, you should create a CR of the Kind &lt;code&gt;CSIPowerMax&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Here are the steps involved in installing a driver:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Make sure all dependencies for the driver have been met. This can include installation of specific packages, configuration of services, nodes&lt;/li&gt;
&lt;li&gt;Create the namespace where you wish to install the driver&lt;/li&gt;
&lt;li&gt;Create any mandatory and optional secrets required for the driver installation&lt;/li&gt;
&lt;li&gt;Create the Custom Resource using the sample manifests provided for the driver&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;create-custom-resource&#34;&gt;Create custom resource&lt;/h3&gt;
&lt;p&gt;A lot of sample manifest files have been provided in the &lt;code&gt;deploy/crd&lt;/code&gt; folder to help with the installation of various CSI Drivers
They follow the naming convention&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{driver name}_{driver version}_k8s_{k8 version}.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{driver name}_{driver version}_ops_{OpenShift version}.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Use the correct sample manifest based on the driver, driver version and Kubernetes/OpenShift version&lt;/p&gt;
&lt;p&gt;For e.g.&lt;br&gt;
powermax_v140_k8s_117.yaml &amp;lt;- To install CSI PowerMax driver v1.4.0 on a Kubernetes 1.17 cluster
powermax_v140_ops_43.yaml &amp;lt;- To install CSI PowerMax driver v1.4.0 on an OpenShift 4.3 cluster&lt;/p&gt;
&lt;p&gt;Note - For this example, we will assume that the Custom Resource will be created in the namespace &lt;code&gt;powermax&lt;/code&gt; with the name &lt;code&gt;powermax&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Create a new file &lt;code&gt;powermax.yaml&lt;/code&gt; by copying the relevant sample file and edit the contents (specific to your installation).&lt;/p&gt;
&lt;p&gt;Run the command &lt;code&gt;kubectl create -f powermax.yaml&lt;/code&gt; to create the Custom Resource (CR)&lt;/p&gt;
&lt;p&gt;Check the status of the Custom Resource by running the command &lt;code&gt;kubectl get csipowermax -n powermax&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;If the status of the CR shows &lt;code&gt;Running&lt;/code&gt; then the driver installation completed successfully with all driver pods running&lt;br&gt;
If the status of the CR shows &lt;code&gt;Succeeded&lt;/code&gt; then the driver installation succeeded but all driver pods are not up and running&lt;br&gt;
If the status of the CR shows &lt;code&gt;InvalidConfig&lt;/code&gt; then there is an incorrect value specified in the Custom Resource manifest&lt;/p&gt;
&lt;p&gt;Note - The driver status can take some time to migrate from &lt;code&gt;Succeeded&lt;/code&gt; to &lt;code&gt;Running&lt;/code&gt; because of the time taken for the driver pods to completely start up.&lt;br&gt;
In case some pods are not up and running, the Operator will query for their status for at least an hour before giving up on updating the status of the Custom Resource.&lt;/p&gt;
&lt;p&gt;You can also check the status of the driver pods by running the command &lt;code&gt;kubectl get pods -n powermax&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Note - Run the command &lt;code&gt;kubectl get csipowermax --all-namespaces&lt;/code&gt; to query for all Custom Resources of the type CSIPowerMax in your cluster&lt;/p&gt;
&lt;h3 id=&#34;update-custom-resource&#34;&gt;Update Custom Resource&lt;/h3&gt;
&lt;p&gt;If you want to update the driver installation or fix any issues in the Custom Resource (for e.g. - InValidConfig), then you can update the Custom Resource&lt;br&gt;
This can be done in multiple ways&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Run the command &lt;code&gt;kubectl edit csipowermax powermax -n powermax&lt;/code&gt; and edit any desired field(s)&lt;/li&gt;
&lt;li&gt;Update the manifest file and run the command &lt;code&gt;kubectl apply -f powermax.yaml&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Directly patch the Custom Resource (refer Kubernetes documentation)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Once the update has been applied to the Custom Resource, the Operator will try to &lt;code&gt;reconcile&lt;/code&gt; the desired state with the observed state in the cluster and apply required changes (if any) to the various resources part of the driver installation&lt;/p&gt;
&lt;h3 id=&#34;delete-custom-resource&#34;&gt;Delete Custom Resource&lt;/h3&gt;
&lt;p&gt;Run the command &lt;code&gt;kubectl delete -f powermax.yaml&lt;/code&gt; to delete the Custom Resource. This will delete the Custom Resource and delete all the driver pods.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Installation process of specific driver</title>
      <link>https://sakshimakkar.github.io/hugo1/docs/getting-started/driver/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://sakshimakkar.github.io/hugo1/docs/getting-started/driver/</guid>
      <description>
        
        
        
      </description>
    </item>
    
  </channel>
</rss>
